{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JwWiaR3XHfH",
        "outputId": "a8340be0-e46a-47df-deed-0d594ad6cf8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Searching and retrieving answer...\n",
            "[Search] Time taken: 2.185s\n",
            "[Content Fetch] Time taken: 0.218s\n",
            "[Text Split] Time taken: 0.002s\n",
            "[Embedding + DB Build] Time taken: 0.270s\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "[LLM Inference] Time taken: 4.697s\n",
            "[LLM] Estimated time per token: 0.427s\n",
            "[Total Research Query] Total time: 4.968s\n",
            "\n",
            "===== Answer =====\n",
            "India won the Asia Cup 2025, defeating Pakistan in the final.\n",
            "[Total pipeline] Total time: 7.372s\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "import time\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.document_loaders import UnstructuredURLLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "# -----------------------------\n",
        "# Config variables\n",
        "# -----------------------------\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "You are a great researcher. With the information provided understand in deep and try to answer the question.\n",
        "If you cant answer the question based on the information either say you cant find an answer or unable to find an answer.\n",
        "So try to understand in depth about the context and answer only based on the information provided. Dont generate irrelevant answers.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Do provide only helpful answers\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "INPUT_VARIABLES = [\"context\", \"question\"]\n",
        "SEPARATORS = \"\\n\"\n",
        "CHUNK_SIZE = 100\n",
        "CHUNK_OVERLAP = 50\n",
        "EMBEDDER = \"BAAI/bge-base-en-v1.5\"\n",
        "CHAIN_TYPE = \"stuff\"\n",
        "SEARCH_KWARGS = {'k': 2}\n",
        "\n",
        "# -----------------------------\n",
        "# Researcher class\n",
        "# -----------------------------\n",
        "class Researcher:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.serper_api_key = os.getenv(\"SERPER_API_KEY\")\n",
        "        self.groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "        self.prompt_template = PromptTemplate(\n",
        "            template=PROMPT_TEMPLATE,\n",
        "            input_variables=INPUT_VARIABLES\n",
        "        )\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            separators=SEPARATORS,\n",
        "            chunk_size=CHUNK_SIZE,\n",
        "            chunk_overlap=CHUNK_OVERLAP\n",
        "        )\n",
        "        self.llm = ChatGroq(\n",
        "            temperature=0.5,\n",
        "            model_name=\"groq/compound\",\n",
        "            groq_api_key=self.groq_api_key\n",
        "        )\n",
        "\n",
        "        # -----------------------------\n",
        "        # GPU-based embeddings\n",
        "        # -----------------------------\n",
        "        self.hfembeddings = HuggingFaceEmbeddings(\n",
        "            model_name=EMBEDDER,\n",
        "            model_kwargs={'device': 'cuda'}  # Use GPU\n",
        "        )\n",
        "\n",
        "    # -----------------------------\n",
        "    # Search articles using Serper API\n",
        "    # -----------------------------\n",
        "    def search_articles(self, query):\n",
        "        t0 = time.time()\n",
        "        url = \"https://google.serper.dev/search\"\n",
        "        data = json.dumps({\"q\": query})\n",
        "        headers = {\n",
        "            'X-API-KEY': self.serper_api_key,\n",
        "            'Content-Type': 'application/json'\n",
        "        }\n",
        "        response = requests.post(url, headers=headers, data=data)\n",
        "        t1 = time.time()\n",
        "        print(f\"[Search] Time taken: {t1-t0:.3f}s\")\n",
        "        return response.json(), t1-t0\n",
        "\n",
        "    # -----------------------------\n",
        "    # Extract URLs from search results\n",
        "    # -----------------------------\n",
        "    def get_urls(self, articles):\n",
        "        urls = []\n",
        "        try:\n",
        "            urls.append(articles[\"answerBox\"][\"link\"])\n",
        "        except:\n",
        "            pass\n",
        "        for i in range(0, min(3, len(articles.get(\"organic\", [])))):\n",
        "            urls.append(articles[\"organic\"][i][\"link\"])\n",
        "        return urls\n",
        "\n",
        "    # -----------------------------\n",
        "    # Fetch content from URLs\n",
        "    # -----------------------------\n",
        "    def get_content_from_urls(self, urls):\n",
        "        t0 = time.time()\n",
        "        loader = UnstructuredURLLoader(urls=urls)\n",
        "        try:\n",
        "            research_content = loader.load()\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading URLs: {e}\")\n",
        "            research_content = []\n",
        "        t1 = time.time()\n",
        "        print(f\"[Content Fetch] Time taken: {t1-t0:.3f}s\")\n",
        "        return research_content, t1-t0\n",
        "\n",
        "    # -----------------------------\n",
        "    # Process query: embeddings, FAISS, LLM\n",
        "    # -----------------------------\n",
        "    def research_given_query(self, research_objective, research_content):\n",
        "        t0_total = time.time()\n",
        "\n",
        "        # Split content into chunks\n",
        "        docs = self.text_splitter.split_documents(research_content)\n",
        "        t_split = time.time()\n",
        "        print(f\"[Text Split] Time taken: {t_split - t0_total:.3f}s\")\n",
        "\n",
        "        # Create FAISS vector store using GPU embeddings\n",
        "        self.db = FAISS.from_documents(documents=docs, embedding=self.hfembeddings)\n",
        "        t_embed = time.time()\n",
        "        print(f\"[Embedding + DB Build] Time taken: {t_embed - t_split:.3f}s\")\n",
        "\n",
        "        # Setup RetrievalQA chain\n",
        "        bot = RetrievalQA.from_chain_type(\n",
        "            llm=self.llm,\n",
        "            chain_type=CHAIN_TYPE,\n",
        "            retriever=self.db.as_retriever(search_kwargs=SEARCH_KWARGS),\n",
        "            return_source_documents=True,\n",
        "            verbose=True,\n",
        "            chain_type_kwargs={\"prompt\": self.prompt_template}\n",
        "        )\n",
        "\n",
        "        # LLM inference\n",
        "        t_start_infer = time.time()\n",
        "        research_out = bot.invoke({\"query\": research_objective})\n",
        "        t_end_infer = time.time()\n",
        "        print(f\"[LLM Inference] Time taken: {t_end_infer - t_start_infer:.3f}s\")\n",
        "\n",
        "        # Estimate time per token\n",
        "        if 'result' in research_out and isinstance(research_out['result'], str):\n",
        "            num_tokens = len(research_out['result'].split())\n",
        "            print(f\"[LLM] Estimated time per token: {(t_end_infer - t_start_infer)/num_tokens:.3f}s\")\n",
        "\n",
        "        t_total = time.time()\n",
        "        print(f\"[Total Research Query] Total time: {t_total - t0_total:.3f}s\")\n",
        "\n",
        "        return research_out[\"result\"]\n",
        "\n",
        "    # -----------------------------\n",
        "    # Full research pipeline\n",
        "    # -----------------------------\n",
        "    def research(self, query):\n",
        "        search_articles, t_search = self.search_articles(query)\n",
        "        urls = self.get_urls(search_articles)\n",
        "        research_content, t_content = self.get_content_from_urls(urls)\n",
        "        answer = self.research_given_query(query, research_content)\n",
        "        return answer\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Run example\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    os.environ[\"SERPER_API_KEY\"] = \"\"\n",
        "    os.environ[\"GROQ_API_KEY\"] = \"\"\n",
        "\n",
        "    researcher = Researcher()\n",
        "    query = \"Who won Asia Cup 2025?\"\n",
        "    print(\"Searching and retrieving answer...\")\n",
        "\n",
        "    t_start = time.time()\n",
        "    answer = researcher.research(query)\n",
        "    t_end = time.time()\n",
        "\n",
        "    print(\"\\n===== Answer =====\")\n",
        "    print(answer)\n",
        "    print(f\"[Total pipeline] Total time: {t_end - t_start:.3f}s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6e627e4"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain langchain_groq langchain_community langchain_huggingface langchain-text-splitters python-dotenv unstructured faiss-cpu sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQCpFmlR_m_n",
        "outputId": "d2d42ee5-5fae-4ba1-a10a-ca4e933c956f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Using device: cuda for embeddings\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2209288839.py:199: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  retrieved = retriever.get_relevant_documents(q)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1] Query: Who is the CEO of NVIDIA?, Result: hit, Time: 2.045s\n",
            "[2] Query: What is the capital of Japan?, Result: hit, Time: 3.002s\n",
            "[3] Query: When was OpenAI founded?, Result: hit, Time: 2.698s\n",
            "[4] Query: What is LangChain used for?, Result: hit, Time: 2.847s\n",
            "[5] Query: Which company created the Mistral-7B model?, Result: hit, Time: 1.442s\n",
            "[6] Query: What is FAISS used for?, Result: hit, Time: 2.763s\n",
            "[7] Query: Who invented the World Wide Web?, Result: hit, Time: 4.127s\n",
            "[8] Query: What is HuggingFace Transformers?, Result: miss, Time: 2.258s\n",
            "[9] Query: What is the default port for HTTP?, Result: hit, Time: 0.956s\n",
            "[10] Query: What programming language is TensorFlow written in?, Result: hit, Time: 1.337s\n",
            "[11] Query: What is the latest iPhone model?, Result: hit, Time: 1.714s\n",
            "[12] Query: Which algorithm does Google use for search ranking?, Result: hit, Time: 1.919s\n",
            "[13] Query: Let x₀ be the real number such that e^(x₀) + x₀ = 0. For a given real number α, define g(x) = (3 x e^x + 3 x - α e^x - α x) / (3 (e^x + 1)) for all real numbers x. Then which one of the following statements is TRUE? (A) For α = 2, lim x→x₀ |g(x) + e^(x₀)| / |x - x₀| = 0. (B) For α = 2, lim x→x₀ |g(x) + e^(x₀)| / |x - x₀| = 1. (C) For α = 3, lim x→x₀ |g(x) + e^(x₀)| / |x - x₀| = 0. (D) For α = 3, lim x→x₀ |g(x) + e^(x₀)| / |x - x₀| = 2/3., Result: hit, Time: 3.144s\n",
            "[14] Query: Give me Optiver OA questions that happened on 14th October 2025, Result: miss, Time: 2.422s\n",
            "Retrieval accuracy @top-3: 85.71% (12/14)\n",
            "Response time summary (s): mean=2.334, median=2.340, max=4.127\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# Config variables (inline instead of config.py)\n",
        "# -----------------------------\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "You are a great researcher. With the information provided understand in deep and try to answer the question.\n",
        "If you cant answer the question based on the information either say you cant find an answer or unable to find an answer.\n",
        "So try to understand in depth about the context and answer only based on the information provided. Dont generate irrelevant answers.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Do provide only helpful answers\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "INPUT_VARIABLES = [\"context\", \"question\"]\n",
        "SEPARATORS = \"\\n\"\n",
        "CHUNK_SIZE = 3000\n",
        "CHUNK_OVERLAP = 500\n",
        "EMBEDDER = \"BAAI/bge-base-en-v1.5\"\n",
        "CHAIN_TYPE = \"stuff\"\n",
        "SEARCH_KWARGS = {'k': 3}\n",
        "\n",
        "# -----------------------------\n",
        "# Imports\n",
        "# -----------------------------\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "from typing import List\n",
        "import time\n",
        "import statistics\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.document_loaders import UnstructuredURLLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "import torch\n",
        "\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "# -----------------------------\n",
        "# Researcher class (with GPU FAISS)\n",
        "# -----------------------------\n",
        "class Researcher:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.serper_api_key = os.getenv(\"SERPER_API_KEY\")\n",
        "        self.groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "        self.prompt_template = PromptTemplate(\n",
        "            template=PROMPT_TEMPLATE,\n",
        "            input_variables=INPUT_VARIABLES\n",
        "        )\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            separators=SEPARATORS,\n",
        "            chunk_size=CHUNK_SIZE,\n",
        "            chunk_overlap=CHUNK_OVERLAP\n",
        "        )\n",
        "\n",
        "        # Use a current production Groq model\n",
        "        self.llm = ChatGroq(\n",
        "            temperature=0.5,\n",
        "            model_name=\"groq/compound\",\n",
        "            groq_api_key=self.groq_api_key\n",
        "        )\n",
        "\n",
        "        # GPU-friendly embeddings\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.hfembeddings = HuggingFaceEmbeddings(\n",
        "            model_name=EMBEDDER,\n",
        "            model_kwargs={'device': device}\n",
        "        )\n",
        "        print(f\"[INFO] Using device: {device} for embeddings\")\n",
        "\n",
        "    def search_articles(self, query: str) -> dict:\n",
        "        url = \"https://google.serper.dev/search\"\n",
        "        data = json.dumps({\"q\": query})\n",
        "        headers = {\n",
        "            'X-API-KEY': self.serper_api_key,\n",
        "            'Content-Type': 'application/json'\n",
        "        }\n",
        "        response = requests.post(url, headers=headers, data=data)\n",
        "        response.raise_for_status()\n",
        "        return response.json()\n",
        "\n",
        "    def research_answerer(self):\n",
        "        research_qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=self.llm,\n",
        "            chain_type=CHAIN_TYPE,\n",
        "            retriever=self.db.as_retriever(search_kwargs=SEARCH_KWARGS),\n",
        "            return_source_documents=True,\n",
        "            verbose=True,\n",
        "            chain_type_kwargs={\"prompt\": self.prompt_template}\n",
        "        )\n",
        "        return research_qa_chain\n",
        "\n",
        "    def get_urls(self, articles: dict) -> List[str]:\n",
        "        urls = []\n",
        "        if not articles:\n",
        "            return urls\n",
        "        try:\n",
        "            if \"answerBox\" in articles and \"link\" in articles[\"answerBox\"]:\n",
        "                urls.append(articles[\"answerBox\"][\"link\"])\n",
        "        except Exception:\n",
        "            pass\n",
        "        for i in range(0, min(3, len(articles.get(\"organic\", [])))):\n",
        "            urls.append(articles[\"organic\"][i][\"link\"])\n",
        "        return urls\n",
        "\n",
        "    def get_content_from_urls(self, urls: List[str]):\n",
        "        documents = []\n",
        "        for url in urls:\n",
        "            try:\n",
        "                loader = UnstructuredURLLoader(urls=[url])\n",
        "                docs = loader.load()\n",
        "                for d in docs:\n",
        "                    if not d.metadata.get(\"source\"):\n",
        "                        d.metadata[\"source\"] = url\n",
        "                documents.extend(docs)\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping URL {url}: {e}\")\n",
        "        return documents\n",
        "\n",
        "    def research_given_query(self, research_objective: str, research_content):\n",
        "        docs = self.text_splitter.split_documents(research_content)\n",
        "\n",
        "        # Use FAISS GPU index if CUDA available\n",
        "        try:\n",
        "            import faiss\n",
        "            index = FAISS.from_documents(documents=docs, embedding=self.hfembeddings, index_factory=\"Flat\")\n",
        "            if torch.cuda.is_available():\n",
        "                gpu_res = faiss.StandardGpuResources()\n",
        "                index.index = faiss.index_cpu_to_gpu(gpu_res, 0, index.index)\n",
        "            self.db = index\n",
        "            print(\"[INFO] FAISS GPU index built successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] FAISS GPU index failed, fallback to CPU: {e}\")\n",
        "            self.db = FAISS.from_documents(documents=docs, embedding=self.hfembeddings)\n",
        "\n",
        "        bot = self.research_answerer()\n",
        "        try:\n",
        "            research_out = bot.invoke({\"query\": research_objective})\n",
        "        except Exception:\n",
        "            research_out = bot({\"query\": research_objective})\n",
        "        return research_out[\"result\"]\n",
        "\n",
        "    def research(self, query: str):\n",
        "        search_articles = self.search_articles(query)\n",
        "        urls = self.get_urls(search_articles)\n",
        "        research_content = self.get_content_from_urls(urls)\n",
        "        answer = self.research_given_query(query, research_content)\n",
        "        return answer\n",
        "\n",
        "    # Evaluation method same as your previous code\n",
        "    def evaluate_retriever(self, dataset: List[dict], top_k: int = 3):\n",
        "        results = []\n",
        "        correct = 0\n",
        "        total = len(dataset)\n",
        "        times = []\n",
        "\n",
        "        for i, sample in enumerate(dataset, start=1):\n",
        "            q = sample[\"query\"]\n",
        "            expected_source = sample.get(\"expected_source\", \"\").lower()\n",
        "            expected_answer = sample.get(\"expected_answer\", \"\").lower()\n",
        "\n",
        "            start_time = time.time()\n",
        "            try:\n",
        "                search_res = self.search_articles(q)\n",
        "            except Exception as e:\n",
        "                elapsed = time.time() - start_time\n",
        "                times.append(elapsed)\n",
        "                results.append((q, False, [], \"search_failed\", elapsed))\n",
        "                continue\n",
        "\n",
        "            urls = self.get_urls(search_res)\n",
        "            docs = self.get_content_from_urls(urls)\n",
        "            if not docs:\n",
        "                elapsed = time.time() - start_time\n",
        "                times.append(elapsed)\n",
        "                results.append((q, False, urls, \"no_docs\", elapsed))\n",
        "                continue\n",
        "\n",
        "            # FAISS GPU index\n",
        "            try:\n",
        "                import faiss\n",
        "                db = FAISS.from_documents(documents=docs, embedding=self.hfembeddings, index_factory=\"Flat\")\n",
        "                if torch.cuda.is_available():\n",
        "                    gpu_res = faiss.StandardGpuResources()\n",
        "                    db.index = faiss.index_cpu_to_gpu(gpu_res, 0, db.index)\n",
        "                retriever = db.as_retriever(search_kwargs={\"k\": top_k})\n",
        "            except Exception as e:\n",
        "                db = FAISS.from_documents(documents=docs, embedding=self.hfembeddings)\n",
        "                retriever = db.as_retriever(search_kwargs={\"k\": top_k})\n",
        "\n",
        "            try:\n",
        "                retrieved = retriever.get_relevant_documents(q)\n",
        "            except Exception as e:\n",
        "                elapsed = time.time() - start_time\n",
        "                times.append(elapsed)\n",
        "                results.append((q, False, [], \"retrieval_failed\", elapsed))\n",
        "                continue\n",
        "\n",
        "            retrieved_sources = []\n",
        "            found = False\n",
        "            for d in retrieved:\n",
        "                src = \"\"\n",
        "                for key in (\"source\", \"url\", \"source_url\"):\n",
        "                    if key in d.metadata and d.metadata[key]:\n",
        "                        src = str(d.metadata[key]).lower()\n",
        "                        break\n",
        "                retrieved_sources.append(src)\n",
        "                content = (d.page_content or \"\").lower()\n",
        "                if expected_source and expected_source in src:\n",
        "                    found = True\n",
        "                    break\n",
        "                if expected_answer and expected_answer in content:\n",
        "                    found = True\n",
        "                    break\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            times.append(elapsed)\n",
        "            if found:\n",
        "                correct += 1\n",
        "                status = \"hit\"\n",
        "            else:\n",
        "                status = \"miss\"\n",
        "\n",
        "            results.append((q, found, urls, retrieved_sources, status, elapsed))\n",
        "            print(f\"[{i}] Query: {q}, Result: {status}, Time: {elapsed:.3f}s\")\n",
        "\n",
        "        accuracy = correct / total * 100 if total else 0.0\n",
        "        print(f\"Retrieval accuracy @top-{top_k}: {accuracy:.2f}% ({correct}/{total})\")\n",
        "        if times:\n",
        "            print(f\"Response time summary (s): mean={statistics.mean(times):.3f}, median={statistics.median(times):.3f}, max={max(times):.3f}\")\n",
        "        return accuracy, results, times\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Test dataset (14 likely-success cases + 2 likely-paywalled/failure cases = 16)\n",
        "# -----------------------------\n",
        "TEST_DATA = [\n",
        "    {\"query\": \"Who is the CEO of NVIDIA?\", \"expected_answer\": \"jensen huang\", \"expected_source\": \"nvidia.com\"},\n",
        "    {\"query\": \"What is the capital of Japan?\", \"expected_answer\": \"tokyo\", \"expected_source\": \"en.wikipedia.org/wiki/Tokyo\"},\n",
        "    {\"query\": \"When was OpenAI founded?\", \"expected_answer\": \"december 2015\", \"expected_source\": \"en.wikipedia.org/wiki/OpenAI\"},\n",
        "    {\"query\": \"What is LangChain used for?\", \"expected_answer\": \"framework for building applications\", \"expected_source\": \"langchain.com\"},\n",
        "    {\"query\": \"Which company created the Mistral-7B model?\", \"expected_answer\": \"mistral ai\", \"expected_source\": \"mistral.ai\"},\n",
        "    {\"query\": \"What is FAISS used for?\", \"expected_answer\": \"similarity search\", \"expected_source\": \"github.com/facebookresearch/faiss\"},\n",
        "    {\"query\": \"Who invented the World Wide Web?\", \"expected_answer\": \"tim berners-lee\", \"expected_source\": \"en.wikipedia.org/wiki/Tim_Berners-Lee\"},\n",
        "    {\"query\": \"What is HuggingFace Transformers?\", \"expected_answer\": \"library for state-of-the-art natural language processing models\", \"expected_source\": \"huggingface.co/transformers\"},\n",
        "    {\"query\": \"What is the default port for HTTP?\", \"expected_answer\": \"port 80\", \"expected_source\": \"developer.mozilla.org\"},\n",
        "    {\"query\": \"What programming language is TensorFlow written in?\", \"expected_answer\": \"c++\", \"expected_source\": \"tensorflow.org\"},\n",
        "    {\"query\": \"What is the latest iPhone model?\", \"expected_answer\": \"iphone 16\", \"expected_source\": \"apple.com\"},\n",
        "    {\"query\": \"Which algorithm does Google use for search ranking?\", \"expected_answer\": \"pagerank\", \"expected_source\": \"en.wikipedia.org/wiki/PageRank\"},\n",
        "    # 2 likely-paywalled / fail cases (Chegg, Course Hero) to simulate sources that often block scraping/paywall\n",
        "    {\n",
        "    \"query\": \"Let x₀ be the real number such that e^(x₀) + x₀ = 0. For a given real number α, define g(x) = (3 x e^x + 3 x - α e^x - α x) / (3 (e^x + 1)) for all real numbers x. Then which one of the following statements is TRUE? (A) For α = 2, lim x→x₀ |g(x) + e^(x₀)| / |x - x₀| = 0. (B) For α = 2, lim x→x₀ |g(x) + e^(x₀)| / |x - x₀| = 1. (C) For α = 3, lim x→x₀ |g(x) + e^(x₀)| / |x - x₀| = 0. (D) For α = 3, lim x→x₀ |g(x) + e^(x₀)| / |x - x₀| = 2/3.\",\n",
        "    \"expected_answer\": \"C\",\n",
        "    \"expected_source\": \"sarthaks.com\"\n",
        "    },\n",
        "    {\"query\": \"Give me Optiver OA questions that happened on 14th October 2025\", \"expected_answer\": \"Researchers are studying the behavior of squirrels in a forest, where they compete to hide and collect nuts in hidden locations. You are tasked with implementing a tracker that can simulate this behavior. Your system is initialized with a list of locations and their capacities, and should support the following actions: Register a nut has been hid by a squirrel in a location. Register how nuts are retrieved by a squirrel from a location. Complete the functions described below in the SquirrelResearch class.\"\n",
        "    , \"expected_source\": \"https://oahelper.in/questions/U2FsdGVkX18gVO7LQ0Ng-51OZtY-etlab8ViFBPseMw?company_id=U2FsdGVkX19j-__NQxYfL1WkTv_vpON0lGt2of6ikmU\"}\n",
        "]\n",
        "\n",
        "# -----------------------------\n",
        "# Run evaluation if executed directly\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Provide your API keys here (or ensure .env is present)\n",
        "    os.environ[\"SERPER_API_KEY\"] = \"\"\n",
        "    os.environ[\"GROQ_API_KEY\"] = \"\"\n",
        "\n",
        "    researcher = Researcher()\n",
        "\n",
        "    # Run the retriever-only evaluation (does NOT invoke the LLM for scoring)\n",
        "    acc, details, timings = researcher.evaluate_retriever(TEST_DATA, top_k=3)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
