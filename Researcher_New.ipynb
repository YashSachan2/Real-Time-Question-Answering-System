{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmqBmMbhJe57",
        "outputId": "c98d76f2-2390-426e-a5a1-71a87813c6fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Searching and retrieving answer...\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "===== Answer =====\n",
            "Based on the provided context, the answer to the question \"Who won Asia Cup 2025?\" is:\n",
            "\n",
            "India\n",
            "\n",
            "This information is directly mentioned in the context, specifically in the article \"Why did India refuse to accept the Asia Cup trophy after beating Pakistan?\" which states: \"Tensions between India and Pakistan scorched the world of sports on Sunday when the Indian cricket team refused to accept the Asia Cup trophy after beating their neighbours by five wickets in the final in Dubai.\" \n",
            "\n",
            "Additionally, the same information is also mentioned in the article \"Asia Cup 2025: India and Pakistan turn cricket into militarised theatre\" which states: \"Despite being fined for making comments deemed political – dedicating India’s win to the victims of the Pahalgam attack and the Indian armed forces – at a post-match news conference on September 14, India’s captain, Suryakumar Yadav, made similar remarks with even greater passion after India’s victory over Pakistan in the final just two weeks later.\" \n",
            "\n",
            "Therefore, based on the provided context, it is clear that India won the Asia Cup 2025.\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# Config variables (inline instead of config.py)\n",
        "# -----------------------------\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "You are a great researcher. With the information provided understand in deep and try to answer the question.\n",
        "If you cant answer the question based on the information either say you cant find an answer or unable to find an answer.\n",
        "So try to understand in depth about the context and answer only based on the information provided. Dont generate irrelevant answers.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Do provide only helpful answers\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "INPUT_VARIABLES = [\"context\", \"question\"]\n",
        "SEPARATORS = \"\\n\"\n",
        "CHUNK_SIZE = 1000\n",
        "CHUNK_OVERLAP = 500\n",
        "EMBEDDER = \"BAAI/bge-base-en-v1.5\"\n",
        "CHAIN_TYPE = \"stuff\"\n",
        "SEARCH_KWARGS = {'k': 3}\n",
        "\n",
        "# -----------------------------\n",
        "# Imports\n",
        "# -----------------------------\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.document_loaders import UnstructuredURLLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings  # updated\n",
        "\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "# -----------------------------\n",
        "# Researcher class\n",
        "# -----------------------------\n",
        "class Researcher:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.serper_api_key = os.getenv(\"SERPER_API_KEY\")\n",
        "        self.groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "        self.prompt_template = PromptTemplate(\n",
        "            template=PROMPT_TEMPLATE,\n",
        "            input_variables=INPUT_VARIABLES\n",
        "        )\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            separators=SEPARATORS,\n",
        "            chunk_size=CHUNK_SIZE,\n",
        "            chunk_overlap=CHUNK_OVERLAP\n",
        "        )\n",
        "\n",
        "        # Use a current production Groq model\n",
        "        self.llm = ChatGroq(\n",
        "            temperature=0.5,\n",
        "            model_name=\"groq/compound\",  # updated model\n",
        "            groq_api_key=self.groq_api_key\n",
        "        )\n",
        "\n",
        "        # CPU-friendly embeddings (safe for Colab free)\n",
        "        self.hfembeddings = HuggingFaceEmbeddings(\n",
        "            model_name=EMBEDDER,\n",
        "            model_kwargs={'device': 'cpu'}\n",
        "        )\n",
        "\n",
        "    def search_articles(self, query):\n",
        "        url = \"https://google.serper.dev/search\"\n",
        "        data = json.dumps({\"q\": query})\n",
        "        headers = {\n",
        "            'X-API-KEY': self.serper_api_key,\n",
        "            'Content-Type': 'application/json'\n",
        "        }\n",
        "        response = requests.post(url, headers=headers, data=data)\n",
        "        return response.json()\n",
        "\n",
        "    def research_answerer(self):\n",
        "        research_qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=self.llm,\n",
        "            chain_type=CHAIN_TYPE,\n",
        "            retriever=self.db.as_retriever(search_kwargs=SEARCH_KWARGS),\n",
        "            return_source_documents=True,\n",
        "            verbose=True,\n",
        "            chain_type_kwargs={\"prompt\": self.prompt_template}\n",
        "        )\n",
        "        return research_qa_chain\n",
        "\n",
        "    def get_urls(self, articles):\n",
        "        urls = []\n",
        "        try:\n",
        "            urls.append(articles[\"answerBox\"][\"link\"])\n",
        "        except:\n",
        "            pass\n",
        "        for i in range(0, min(3, len(articles.get(\"organic\", [])))):\n",
        "            urls.append(articles[\"organic\"][i][\"link\"])\n",
        "        return urls\n",
        "\n",
        "    def get_content_from_urls(self, urls):\n",
        "        loader = UnstructuredURLLoader(urls=urls)\n",
        "        try:\n",
        "            research_content = loader.load()\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading URLs: {e}\")\n",
        "            research_content = []\n",
        "        return research_content\n",
        "\n",
        "    def research_given_query(self, research_objective, research_content):\n",
        "        docs = self.text_splitter.split_documents(research_content)\n",
        "        self.db = FAISS.from_documents(documents=docs, embedding=self.hfembeddings)\n",
        "        bot = self.research_answerer()\n",
        "        research_out = bot.invoke({\"query\": research_objective})  # updated for latest LangChain\n",
        "        return research_out[\"result\"]\n",
        "\n",
        "    def research(self, query):\n",
        "        search_articles = self.search_articles(query)\n",
        "        urls = self.get_urls(search_articles)\n",
        "        research_content = self.get_content_from_urls(urls)\n",
        "        answer = self.research_given_query(query, research_content)\n",
        "        return answer\n",
        "\n",
        "# -----------------------------\n",
        "# Run example\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Set API keys manually in Colab if not using .env\n",
        "    os.environ[\"SERPER_API_KEY\"] = \"\"\n",
        "    os.environ[\"GROQ_API_KEY\"] = \"\"\n",
        "\n",
        "    researcher = Researcher()\n",
        "    query = \"Who won Asia Cup 2025?\"\n",
        "    print(\"Searching and retrieving answer...\")\n",
        "    answer = researcher.research(query)\n",
        "    print(\"\\n===== Answer =====\")\n",
        "    print(answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6e627e4",
        "outputId": "42587fd5-0abe-4252-c0be-74884b8a9a58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m121.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.8/207.8 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain_groq langchain_community langchain_huggingface langchain-text-splitters python-dotenv unstructured faiss-cpu sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQCpFmlR_m_n",
        "outputId": "2d0e200f-a76f-4be8-ea87-74a88f672086"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1] Query: Who is the CEO of NVIDIA?\n",
            "     expected_source: nvidia.com\n",
            "     urls fetched: ['https://en.wikipedia.org/wiki/Jensen_Huang', 'https://www.linkedin.com/in/jenhsunhuang', 'https://www.forbes.com/profile/jensen-huang/']\n",
            "     retrieved sources: ['https://www.forbes.com/profile/jensen-huang/']\n",
            "     result: hit\n",
            "     response_time: 5.066s\n",
            "\n",
            "[2] Query: What is the capital of Japan?\n",
            "     expected_source: en.wikipedia.org/wiki/Tokyo\n",
            "     urls fetched: ['https://en.wikipedia.org/wiki/Capital_of_Japan', 'https://clintonwhitehouse3.archives.gov/WH/New/Pacific/tokyo.html', 'https://www.britannica.com/place/Tokyo']\n",
            "     retrieved sources: ['https://clintonwhitehouse3.archives.gov/wh/new/pacific/tokyo.html']\n",
            "     result: hit\n",
            "     response_time: 5.694s\n",
            "\n",
            "[3] Query: When was OpenAI founded?\n",
            "     expected_source: en.wikipedia.org/wiki/OpenAI\n",
            "     urls fetched: ['https://en.wikipedia.org/wiki/OpenAI', 'https://www.lxahub.com/stories/the-history-of-openai', 'https://www.opb.org/article/2023/11/24/how-openai-s-origins-explain-the-sam-altman-drama/']\n",
            "     retrieved sources: ['https://www.opb.org/article/2023/11/24/how-openai-s-origins-explain-the-sam-altman-drama/', 'https://www.lxahub.com/stories/the-history-of-openai']\n",
            "     result: hit\n",
            "     response_time: 4.327s\n",
            "\n",
            "[4] Query: What is LangChain used for?\n",
            "     expected_source: langchain.com\n",
            "     urls fetched: ['https://aws.amazon.com/what-is/langchain/', 'https://medium.com/around-the-prompt/what-is-langchain-and-why-should-i-care-as-a-developer-b2d952c42b28', 'https://python.langchain.com/docs/introduction/']\n",
            "     retrieved sources: ['https://aws.amazon.com/what-is/langchain/']\n",
            "     result: hit\n",
            "     response_time: 5.376s\n",
            "\n",
            "[5] Query: Which company created the Mistral-7B model?\n",
            "     expected_source: mistral.ai\n",
            "     urls fetched: ['https://en.wikipedia.org/wiki/Mistral_AI', 'https://mistral.ai/news/announcing-mistral-7b', 'https://huggingface.co/mistralai/Mistral-7B-v0.1']\n",
            "     retrieved sources: ['https://mistral.ai/news/announcing-mistral-7b']\n",
            "     result: hit\n",
            "     response_time: 4.315s\n",
            "\n",
            "[6] Query: What is FAISS used for?\n",
            "     expected_source: github.com/facebookresearch/faiss\n",
            "     urls fetched: ['https://www.pinecone.io/learn/series/faiss/faiss-tutorial/', 'https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/', 'https://www.datacamp.com/blog/faiss-facebook-ai-similarity-search']\n",
            "     retrieved sources: ['https://www.pinecone.io/learn/series/faiss/faiss-tutorial/']\n",
            "     result: hit\n",
            "     response_time: 5.199s\n",
            "\n",
            "[7] Query: Who invented the World Wide Web?\n",
            "     expected_source: en.wikipedia.org/wiki/Tim_Berners-Lee\n",
            "     urls fetched: ['https://en.wikipedia.org/wiki/Tim_Berners-Lee', 'https://home.cern/science/computing/birth-web/short-history-web', 'https://www.home.cern/science/computing/birth-web']\n",
            "     retrieved sources: ['https://www.home.cern/science/computing/birth-web']\n",
            "     result: hit\n",
            "     response_time: 6.202s\n",
            "\n",
            "[8] Query: What is HuggingFace Transformers?\n",
            "     expected_source: huggingface.co/transformers\n",
            "     urls fetched: ['https://huggingface.co/docs/transformers/en/index', 'https://learn.microsoft.com/en-us/azure/databricks/machine-learning/train-model/huggingface/', 'https://github.com/huggingface/transformers']\n",
            "     retrieved sources: ['https://github.com/huggingface/transformers', 'https://huggingface.co/docs/transformers/en/index', 'https://learn.microsoft.com/en-us/azure/databricks/machine-learning/train-model/huggingface/']\n",
            "     result: miss\n",
            "     response_time: 5.270s\n",
            "\n",
            "[9] Query: What is the default port for HTTP?\n",
            "     expected_source: developer.mozilla.org\n",
            "     urls fetched: ['https://superuser.com/questions/996828/why-was-port-80-chosen-as-the-default-http-port-and-443-as-the-default-https-por', 'https://www.cbtnuggets.com/blog/technology/networking/http-80-vs-https-443', 'https://docs.oracle.com/en/storage/tape-storage/sl4000/slklg/default-port-numbers.html']\n",
            "     retrieved sources: ['https://docs.oracle.com/en/storage/tape-storage/sl4000/slklg/default-port-numbers.html', 'https://www.cbtnuggets.com/blog/technology/networking/http-80-vs-https-443']\n",
            "     result: hit\n",
            "     response_time: 4.125s\n",
            "\n",
            "[10] Query: What programming language is TensorFlow written in?\n",
            "     expected_source: tensorflow.org\n",
            "     urls fetched: ['https://stackoverflow.com/questions/35677724/tensorflow-why-was-python-the-chosen-language', 'https://en.wikipedia.org/wiki/TensorFlow', 'https://www.databricks.com/glossary/tensorflow-guide']\n",
            "     retrieved sources: ['https://www.databricks.com/glossary/tensorflow-guide']\n",
            "     result: hit\n",
            "     response_time: 4.457s\n",
            "\n",
            "[11] Query: What is the latest iPhone model?\n",
            "     expected_source: apple.com\n",
            "     urls fetched: ['https://www.apple.com/iphone/', 'https://www.apple.com/iphone-17/', 'https://en.wikipedia.org/wiki/List_of_iPhone_models']\n",
            "     retrieved sources: ['https://www.apple.com/iphone-17/']\n",
            "     result: hit\n",
            "     response_time: 5.059s\n",
            "\n",
            "[12] Query: Which algorithm does Google use for search ranking?\n",
            "     expected_source: en.wikipedia.org/wiki/PageRank\n",
            "     urls fetched: ['https://en.wikipedia.org/wiki/PageRank', 'https://www.google.com/intl/en_us/search/howsearchworks/how-search-works/ranking-results', 'https://developers.google.com/search/docs/fundamentals/how-search-works']\n",
            "     retrieved sources: ['https://www.google.com/intl/en_us/search/howsearchworks/how-search-works/ranking-results', 'https://en.wikipedia.org/wiki/pagerank']\n",
            "     result: hit\n",
            "     response_time: 4.609s\n",
            "\n",
            "[13] Query: Let x₀ be the real number such that e^(x₀) + x₀ = 0. For a given real number α, define g(x) = (3 x e^x + 3 x - α e^x - α x) / (3 (e^x + 1)) for all real numbers x. Then which one of the following statements is TRUE? (A) For α = 2, lim x→x₀ |g(x) + e^(x₀)| / |x - x₀| = 0. (B) For α = 2, lim x→x₀ |g(x) + e^(x₀)| / |x - x₀| = 1. (C) For α = 3, lim x→x₀ |g(x) + e^(x₀)| / |x - x₀| = 0. (D) For α = 3, lim x→x₀ |g(x) + e^(x₀)| / |x - x₀| = 2/3.\n",
            "     expected_source: sarthaks.com\n",
            "     urls fetched: ['https://www.youtube.com/watch?v=FhjLg1bFZK4', 'https://www.youtube.com/watch?v=s1ejfgsAQ4Y', 'https://www.sarthaks.com/3792981/let-x0-be-the-real-number-such-that-e-x0-x0-0']\n",
            "     retrieved sources: ['https://www.youtube.com/watch?v=fhjlg1bfzk4']\n",
            "     result: hit\n",
            "     response_time: 4.422s\n",
            "\n",
            "[14] Query: Give me Optiver OA questions that happened on 14th October 2025\n",
            "     expected_source: https://oahelper.in/questions/U2FsdGVkX18gVO7LQ0Ng-51OZtY-etlab8ViFBPseMw?company_id=U2FsdGVkX19j-__NQxYfL1WkTv_vpON0lGt2of6ikmU\n",
            "     urls fetched: ['https://www.reddit.com/r/csMajors/comments/1e954ig/optiver_summer_2025_intern_oa_part_1/', 'https://programhelp.net/en/optiver-oa-optiver-oa-questions-optiver-oa-2025/', 'https://www.glassdoor.com/Interview/Optiver-Interview-Questions-E243355.htm']\n",
            "     retrieved sources: ['https://programhelp.net/en/optiver-oa-optiver-oa-questions-optiver-oa-2025/', 'https://www.reddit.com/r/csmajors/comments/1e954ig/optiver_summer_2025_intern_oa_part_1/', 'https://www.glassdoor.com/interview/optiver-interview-questions-e243355.htm']\n",
            "     result: miss\n",
            "     response_time: 5.725s\n",
            "\n",
            "Retrieval accuracy @top-3: 85.71% (12/14)\n",
            "Response time summary (s): mean=4.989, median=5.062, p95~=6.202\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# Config variables (inline instead of config.py)\n",
        "# -----------------------------\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "You are a great researcher. With the information provided understand in deep and try to answer the question.\n",
        "If you cant answer the question based on the information either say you cant find an answer or unable to find an answer.\n",
        "So try to understand in depth about the context and answer only based on the information provided. Dont generate irrelevant answers.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Do provide only helpful answers\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "INPUT_VARIABLES = [\"context\", \"question\"]\n",
        "SEPARATORS = \"\\n\"\n",
        "CHUNK_SIZE = 3000\n",
        "CHUNK_OVERLAP = 500\n",
        "EMBEDDER = \"BAAI/bge-base-en-v1.5\"\n",
        "CHAIN_TYPE = \"stuff\"\n",
        "SEARCH_KWARGS = {'k': 3}\n",
        "\n",
        "# -----------------------------\n",
        "# Imports\n",
        "# -----------------------------\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "from typing import List\n",
        "import time\n",
        "import statistics\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.document_loaders import UnstructuredURLLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings  # updated\n",
        "\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "# -----------------------------\n",
        "# Researcher class (with evaluation)\n",
        "# -----------------------------\n",
        "class Researcher:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.serper_api_key = os.getenv(\"SERPER_API_KEY\")\n",
        "        self.groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "        self.prompt_template = PromptTemplate(\n",
        "            template=PROMPT_TEMPLATE,\n",
        "            input_variables=INPUT_VARIABLES\n",
        "        )\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            separators=SEPARATORS,\n",
        "            chunk_size=CHUNK_SIZE,\n",
        "            chunk_overlap=CHUNK_OVERLAP\n",
        "        )\n",
        "\n",
        "        # Use a current production Groq model\n",
        "        self.llm = ChatGroq(\n",
        "            temperature=0.5,\n",
        "            model_name=\"groq/compound\",\n",
        "            groq_api_key=self.groq_api_key\n",
        "        )\n",
        "\n",
        "        # CPU-friendly embeddings (safe for Colab free)\n",
        "        self.hfembeddings = HuggingFaceEmbeddings(\n",
        "            model_name=EMBEDDER,\n",
        "            model_kwargs={'device': 'cpu'}\n",
        "        )\n",
        "\n",
        "    def search_articles(self, query: str) -> dict:\n",
        "        url = \"https://google.serper.dev/search\"\n",
        "        data = json.dumps({\"q\": query})\n",
        "        headers = {\n",
        "            'X-API-KEY': self.serper_api_key,\n",
        "            'Content-Type': 'application/json'\n",
        "        }\n",
        "        response = requests.post(url, headers=headers, data=data)\n",
        "        response.raise_for_status()\n",
        "        return response.json()\n",
        "\n",
        "    def research_answerer(self):\n",
        "        research_qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=self.llm,\n",
        "            chain_type=CHAIN_TYPE,\n",
        "            retriever=self.db.as_retriever(search_kwargs=SEARCH_KWARGS),\n",
        "            return_source_documents=True,\n",
        "            verbose=True,\n",
        "            chain_type_kwargs={\"prompt\": self.prompt_template}\n",
        "        )\n",
        "        return research_qa_chain\n",
        "\n",
        "    def get_urls(self, articles: dict) -> List[str]:\n",
        "        urls = []\n",
        "        if not articles:\n",
        "            return urls\n",
        "        try:\n",
        "            if \"answerBox\" in articles and \"link\" in articles[\"answerBox\"]:\n",
        "                urls.append(articles[\"answerBox\"][\"link\"])\n",
        "        except Exception:\n",
        "            pass\n",
        "        for i in range(0, min(3, len(articles.get(\"organic\", [])))):\n",
        "            urls.append(articles[\"organic\"][i][\"link\"])\n",
        "        return urls\n",
        "\n",
        "    def get_content_from_urls(self, urls: List[str]):\n",
        "        # load pages one by one to be robust and skip failures\n",
        "        documents = []\n",
        "        for url in urls:\n",
        "            try:\n",
        "                loader = UnstructuredURLLoader(urls=[url])\n",
        "                docs = loader.load()\n",
        "                # attach url to metadata if loader didn't\n",
        "                for d in docs:\n",
        "                    if not d.metadata.get(\"source\"):\n",
        "                        d.metadata[\"source\"] = url\n",
        "                documents.extend(docs)\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping URL {url}: {e}\")\n",
        "        return documents\n",
        "\n",
        "    def research_given_query(self, research_objective: str, research_content):\n",
        "        docs = self.text_splitter.split_documents(research_content)\n",
        "        self.db = FAISS.from_documents(documents=docs, embedding=self.hfembeddings)\n",
        "        bot = self.research_answerer()\n",
        "        # use invoke if your LangChain version expects it\n",
        "        try:\n",
        "            research_out = bot.invoke({\"query\": research_objective})\n",
        "        except Exception:\n",
        "            research_out = bot({\"query\": research_objective})\n",
        "        return research_out[\"result\"]\n",
        "\n",
        "    def research(self, query: str):\n",
        "        search_articles = self.search_articles(query)\n",
        "        urls = self.get_urls(search_articles)\n",
        "        research_content = self.get_content_from_urls(urls)\n",
        "        answer = self.research_given_query(query, research_content)\n",
        "        return answer\n",
        "\n",
        "    # -----------------------------\n",
        "    # New: evaluate retrieval accuracy + response time\n",
        "    # -----------------------------\n",
        "    def evaluate_retriever(self, dataset: List[dict], top_k: int = 3):\n",
        "        \"\"\"\n",
        "        dataset: list of {\"query\":..., \"expected_source\":..., \"expected_answer\":...}\n",
        "        This runs the same search -> scrape -> embed -> faiss -> retrieve pipeline and\n",
        "        checks whether expected_source or expected_answer snippet appears in top-k docs.\n",
        "        Also measures response time per query (search -> retrieval).\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        correct = 0\n",
        "        total = len(dataset)\n",
        "        times = []\n",
        "\n",
        "        for i, sample in enumerate(dataset, start=1):\n",
        "            q = sample[\"query\"]\n",
        "            expected_source = sample.get(\"expected_source\", \"\").lower()\n",
        "            expected_answer = sample.get(\"expected_answer\", \"\").lower()\n",
        "\n",
        "            start_time = time.time()  # measure start of the pipeline\n",
        "\n",
        "            # Run pipeline: search -> get urls -> load content -> build FAISS\n",
        "            try:\n",
        "                search_res = self.search_articles(q)\n",
        "            except Exception as e:\n",
        "                elapsed = time.time() - start_time\n",
        "                times.append(elapsed)\n",
        "                print(f\"[{i}] Search failed for query: {q} -> {e} (time: {elapsed:.3f}s)\")\n",
        "                results.append((q, False, [], \"search_failed\", elapsed))\n",
        "                continue\n",
        "\n",
        "            urls = self.get_urls(search_res)\n",
        "            docs = self.get_content_from_urls(urls)\n",
        "            if not docs:\n",
        "                elapsed = time.time() - start_time\n",
        "                times.append(elapsed)\n",
        "                print(f\"[{i}] No documents loaded for query: {q} (urls: {urls}) (time: {elapsed:.3f}s)\")\n",
        "                results.append((q, False, urls, \"no_docs\", elapsed))\n",
        "                continue\n",
        "\n",
        "            # build faiss for these docs\n",
        "            db = FAISS.from_documents(documents=docs, embedding=self.hfembeddings)\n",
        "            retriever = db.as_retriever(search_kwargs={\"k\": top_k})\n",
        "\n",
        "            # retrieve\n",
        "            try:\n",
        "                retrieved = retriever.get_relevant_documents(q)\n",
        "            except Exception as e:\n",
        "                elapsed = time.time() - start_time\n",
        "                times.append(elapsed)\n",
        "                print(f\"[{i}] Retrieval failed for query: {q} -> {e} (time: {elapsed:.3f}s)\")\n",
        "                results.append((q, False, [], \"retrieval_failed\", elapsed))\n",
        "                continue\n",
        "\n",
        "            # collect retrieved sources/text\n",
        "            retrieved_sources = []\n",
        "            retrieved_texts = []\n",
        "            found = False\n",
        "            for d in retrieved:\n",
        "                src = \"\"\n",
        "                # try common metadata fields\n",
        "                for key in (\"source\", \"url\", \"source_url\"):\n",
        "                    if key in d.metadata and d.metadata[key]:\n",
        "                        src = str(d.metadata[key]).lower()\n",
        "                        break\n",
        "                retrieved_sources.append(src)\n",
        "                content = (d.page_content or \"\").lower()\n",
        "                retrieved_texts.append(content)\n",
        "\n",
        "                # check for expected_source substring\n",
        "                if expected_source and expected_source in src:\n",
        "                    found = True\n",
        "                    break\n",
        "                # fallback: check expected_answer snippet in content\n",
        "                if expected_answer and expected_answer in content:\n",
        "                    found = True\n",
        "                    break\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            times.append(elapsed)\n",
        "\n",
        "            if found:\n",
        "                correct += 1\n",
        "                status = \"hit\"\n",
        "            else:\n",
        "                status = \"miss\"\n",
        "\n",
        "            results.append((q, found, urls, retrieved_sources, status, elapsed))\n",
        "\n",
        "            # print per-query summary\n",
        "            print(f\"[{i}] Query: {q}\")\n",
        "            print(f\"     expected_source: {sample.get('expected_source')}\")\n",
        "            print(f\"     urls fetched: {urls}\")\n",
        "            print(f\"     retrieved sources: {retrieved_sources}\")\n",
        "            print(f\"     result: {status}\")\n",
        "            print(f\"     response_time: {elapsed:.3f}s\\n\")\n",
        "\n",
        "        accuracy = correct / total * 100 if total else 0.0\n",
        "        print(f\"Retrieval accuracy @top-{top_k}: {accuracy:.2f}% ({correct}/{total})\")\n",
        "\n",
        "        # timing summary\n",
        "        if times:\n",
        "            mean_t = statistics.mean(times)\n",
        "            median_t = statistics.median(times)\n",
        "            p95 = statistics.quantiles(times, n=100)[94] if len(times) >= 100 else max(times)  # fallback when <100 measurements\n",
        "            print(f\"Response time summary (s): mean={mean_t:.3f}, median={median_t:.3f}, p95~={p95:.3f}\")\n",
        "\n",
        "        return accuracy, results, times\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Test dataset (14 likely-success cases + 2 likely-paywalled/failure cases = 16)\n",
        "# -----------------------------\n",
        "TEST_DATA = [\n",
        "    {\"query\": \"Who is the CEO of NVIDIA?\", \"expected_answer\": \"jensen huang\", \"expected_source\": \"nvidia.com\"},\n",
        "    {\"query\": \"What is the capital of Japan?\", \"expected_answer\": \"tokyo\", \"expected_source\": \"en.wikipedia.org/wiki/Tokyo\"},\n",
        "    {\"query\": \"When was OpenAI founded?\", \"expected_answer\": \"december 2015\", \"expected_source\": \"en.wikipedia.org/wiki/OpenAI\"},\n",
        "    {\"query\": \"What is LangChain used for?\", \"expected_answer\": \"framework for building applications\", \"expected_source\": \"langchain.com\"},\n",
        "    {\"query\": \"Which company created the Mistral-7B model?\", \"expected_answer\": \"mistral ai\", \"expected_source\": \"mistral.ai\"},\n",
        "    {\"query\": \"What is FAISS used for?\", \"expected_answer\": \"similarity search\", \"expected_source\": \"github.com/facebookresearch/faiss\"},\n",
        "    {\"query\": \"Who invented the World Wide Web?\", \"expected_answer\": \"tim berners-lee\", \"expected_source\": \"en.wikipedia.org/wiki/Tim_Berners-Lee\"},\n",
        "    {\"query\": \"What is HuggingFace Transformers?\", \"expected_answer\": \"library for state-of-the-art natural language processing models\", \"expected_source\": \"huggingface.co/transformers\"},\n",
        "    {\"query\": \"What is the default port for HTTP?\", \"expected_answer\": \"port 80\", \"expected_source\": \"developer.mozilla.org\"},\n",
        "    {\"query\": \"What programming language is TensorFlow written in?\", \"expected_answer\": \"c++\", \"expected_source\": \"tensorflow.org\"},\n",
        "    {\"query\": \"What is the latest iPhone model?\", \"expected_answer\": \"iphone 16\", \"expected_source\": \"apple.com\"},\n",
        "    {\"query\": \"Which algorithm does Google use for search ranking?\", \"expected_answer\": \"pagerank\", \"expected_source\": \"en.wikipedia.org/wiki/PageRank\"},\n",
        "    {\n",
        "    \"query\": \"Let x₀ be the real number such that e^(x₀) + x₀ = 0. For a given real number α, define g(x) = (3 x e^x + 3 x - α e^x - α x) / (3 (e^x + 1)) for all real numbers x. Then which one of the following statements is TRUE? (A) For α = 2, lim x→x₀ |g(x) + e^(x₀)| / |x - x₀| = 0. (B) For α = 2, lim x→x₀ |g(x) + e^(x₀)| / |x - x₀| = 1. (C) For α = 3, lim x→x₀ |g(x) + e^(x₀)| / |x - x₀| = 0. (D) For α = 3, lim x→x₀ |g(x) + e^(x₀)| / |x - x₀| = 2/3.\",\n",
        "    \"expected_answer\": \"C\",\n",
        "    \"expected_source\": \"sarthaks.com\"\n",
        "    },\n",
        "    {\"query\": \"Give me Optiver OA questions that happened on 14th October 2025\", \"expected_answer\": \"Researchers are studying the behavior of squirrels in a forest, where they compete to hide and collect nuts in hidden locations. You are tasked with implementing a tracker that can simulate this behavior. Your system is initialized with a list of locations and their capacities, and should support the following actions: Register a nut has been hid by a squirrel in a location. Register how nuts are retrieved by a squirrel from a location. Complete the functions described below in the SquirrelResearch class.\"\n",
        "    , \"expected_source\": \"https://oahelper.in/questions/U2FsdGVkX18gVO7LQ0Ng-51OZtY-etlab8ViFBPseMw?company_id=U2FsdGVkX19j-__NQxYfL1WkTv_vpON0lGt2of6ikmU\"}\n",
        "]\n",
        "\n",
        "# -----------------------------\n",
        "# Run evaluation if executed directly\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Provide your API keys here (or ensure .env is present)\n",
        "    os.environ[\"SERPER_API_KEY\"] = os.getenv(\"SERPER_API_KEY\", os.environ.get(\"SERPER_API_KEY\", \"\"))\n",
        "    os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", os.environ.get(\"GROQ_API_KEY\", \"\"))\n",
        "\n",
        "    researcher = Researcher()\n",
        "\n",
        "\n",
        "    researcher = Researcher()\n",
        "\n",
        "    # Run the retriever-only evaluation (does NOT invoke the LLM for scoring)\n",
        "    acc, details, timings = researcher.evaluate_retriever(TEST_DATA, top_k=3)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
