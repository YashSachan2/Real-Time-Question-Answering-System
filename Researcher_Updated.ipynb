{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmqBmMbhJe57",
        "outputId": "c98d76f2-2390-426e-a5a1-71a87813c6fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Searching and retrieving answer...\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "===== Answer =====\n",
            "Based on the provided context, the answer to the question \"Who won Asia Cup 2025?\" is:\n",
            "\n",
            "India\n",
            "\n",
            "This information is directly mentioned in the context, specifically in the article \"Why did India refuse to accept the Asia Cup trophy after beating Pakistan?\" which states: \"Tensions between India and Pakistan scorched the world of sports on Sunday when the Indian cricket team refused to accept the Asia Cup trophy after beating their neighbours by five wickets in the final in Dubai.\" \n",
            "\n",
            "Additionally, the same information is also mentioned in the article \"Asia Cup 2025: India and Pakistan turn cricket into militarised theatre\" which states: \"Despite being fined for making comments deemed political – dedicating India’s win to the victims of the Pahalgam attack and the Indian armed forces – at a post-match news conference on September 14, India’s captain, Suryakumar Yadav, made similar remarks with even greater passion after India’s victory over Pakistan in the final just two weeks later.\" \n",
            "\n",
            "Therefore, based on the provided context, it is clear that India won the Asia Cup 2025.\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# Config variables (inline instead of config.py)\n",
        "# -----------------------------\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "You are a great researcher. With the information provided understand in deep and try to answer the question.\n",
        "If you cant answer the question based on the information either say you cant find an answer or unable to find an answer.\n",
        "So try to understand in depth about the context and answer only based on the information provided. Dont generate irrelevant answers.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Do provide only helpful answers\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "INPUT_VARIABLES = [\"context\", \"question\"]\n",
        "SEPARATORS = \"\\n\"\n",
        "CHUNK_SIZE = 3000\n",
        "CHUNK_OVERLAP = 500\n",
        "EMBEDDER = \"BAAI/bge-base-en-v1.5\"\n",
        "CHAIN_TYPE = \"stuff\"\n",
        "SEARCH_KWARGS = {'k': 3}\n",
        "\n",
        "# -----------------------------\n",
        "# Imports\n",
        "# -----------------------------\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.document_loaders import UnstructuredURLLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings  # updated\n",
        "\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "# -----------------------------\n",
        "# Researcher class\n",
        "# -----------------------------\n",
        "class Researcher:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.serper_api_key = os.getenv(\"SERPER_API_KEY\")\n",
        "        self.groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "        self.prompt_template = PromptTemplate(\n",
        "            template=PROMPT_TEMPLATE,\n",
        "            input_variables=INPUT_VARIABLES\n",
        "        )\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            separators=SEPARATORS,\n",
        "            chunk_size=CHUNK_SIZE,\n",
        "            chunk_overlap=CHUNK_OVERLAP\n",
        "        )\n",
        "\n",
        "        # Use a current production Groq model\n",
        "        self.llm = ChatGroq(\n",
        "            temperature=0.5,\n",
        "            model_name=\"groq/compound\",  # updated model\n",
        "            groq_api_key=self.groq_api_key\n",
        "        )\n",
        "\n",
        "        # CPU-friendly embeddings (safe for Colab free)\n",
        "        self.hfembeddings = HuggingFaceEmbeddings(\n",
        "            model_name=EMBEDDER,\n",
        "            model_kwargs={'device': 'cpu'}\n",
        "        )\n",
        "\n",
        "    def search_articles(self, query):\n",
        "        url = \"https://google.serper.dev/search\"\n",
        "        data = json.dumps({\"q\": query})\n",
        "        headers = {\n",
        "            'X-API-KEY': self.serper_api_key,\n",
        "            'Content-Type': 'application/json'\n",
        "        }\n",
        "        response = requests.post(url, headers=headers, data=data)\n",
        "        return response.json()\n",
        "\n",
        "    def research_answerer(self):\n",
        "        research_qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=self.llm,\n",
        "            chain_type=CHAIN_TYPE,\n",
        "            retriever=self.db.as_retriever(search_kwargs=SEARCH_KWARGS),\n",
        "            return_source_documents=True,\n",
        "            verbose=True,\n",
        "            chain_type_kwargs={\"prompt\": self.prompt_template}\n",
        "        )\n",
        "        return research_qa_chain\n",
        "\n",
        "    def get_urls(self, articles):\n",
        "        urls = []\n",
        "        try:\n",
        "            urls.append(articles[\"answerBox\"][\"link\"])\n",
        "        except:\n",
        "            pass\n",
        "        for i in range(0, min(3, len(articles.get(\"organic\", [])))):\n",
        "            urls.append(articles[\"organic\"][i][\"link\"])\n",
        "        return urls\n",
        "\n",
        "    def get_content_from_urls(self, urls):\n",
        "        loader = UnstructuredURLLoader(urls=urls)\n",
        "        try:\n",
        "            research_content = loader.load()\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading URLs: {e}\")\n",
        "            research_content = []\n",
        "        return research_content\n",
        "\n",
        "    def research_given_query(self, research_objective, research_content):\n",
        "        docs = self.text_splitter.split_documents(research_content)\n",
        "        self.db = FAISS.from_documents(documents=docs, embedding=self.hfembeddings)\n",
        "        bot = self.research_answerer()\n",
        "        research_out = bot.invoke({\"query\": research_objective})  # updated for latest LangChain\n",
        "        return research_out[\"result\"]\n",
        "\n",
        "    def research(self, query):\n",
        "        search_articles = self.search_articles(query)\n",
        "        urls = self.get_urls(search_articles)\n",
        "        research_content = self.get_content_from_urls(urls)\n",
        "        answer = self.research_given_query(query, research_content)\n",
        "        return answer\n",
        "\n",
        "# -----------------------------\n",
        "# Run example\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Set API keys manually in Colab if not using .env\n",
        "    os.environ[\"SERPER_API_KEY\"] = \"##\"\n",
        "    os.environ[\"GROQ_API_KEY\"] = \"##\"\n",
        "\n",
        "    researcher = Researcher()\n",
        "    query = \"Who won Asia Cup 2025?\"\n",
        "    print(\"Searching and retrieving answer...\")\n",
        "    answer = researcher.research(query)\n",
        "    print(\"\\n===== Answer =====\")\n",
        "    print(answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6e627e4",
        "outputId": "f26255d0-5df1-4ec6-c4b4-c7611c580dd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/981.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m778.2/981.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.8/207.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain_groq langchain_community langchain_huggingface langchain-text-splitters python-dotenv unstructured faiss-cpu sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1NOTJy6POFM",
        "outputId": "adc85f39-8f64-46ba-ea0b-e4e35955b540"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:langchain_community.document_loaders.url:Error fetching or processing https://devguide.python.org/versions/, exception: 'lxml.etree._ProcessingInstruction' object has no attribute 'is_phrasing'\n",
            "/tmp/ipython-input-450564083.py:181: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  retrieved = retriever.get_relevant_documents(q)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1] Query: What is the latest version of Python?\n",
            "     expected_source: python.org\n",
            "     urls fetched: ['https://www.python.org/downloads/', 'https://devguide.python.org/versions/', 'https://www.python.org/getit/windows/']\n",
            "     retrieved sources: ['https://www.python.org/downloads/']\n",
            "     result: hit\n",
            "\n",
            "[2] Query: Who is the CEO of NVIDIA?\n",
            "     expected_source: nvidia.com\n",
            "     urls fetched: ['https://en.wikipedia.org/wiki/Jensen_Huang', 'http://nvidianews.nvidia.com/bios/jensen-huang', 'https://www.linkedin.com/in/jenhsunhuang']\n",
            "     retrieved sources: ['http://nvidianews.nvidia.com/bios/jensen-huang']\n",
            "     result: hit\n",
            "\n",
            "[3] Query: What is the capital of Japan?\n",
            "     expected_source: en.wikipedia.org/wiki/Tokyo\n",
            "     urls fetched: ['https://en.wikipedia.org/wiki/Capital_of_Japan', 'https://clintonwhitehouse3.archives.gov/WH/New/Pacific/tokyo.html', 'https://medium.com/yamashita-guild/why-tokyo-isnt-legally-the-capital-of-japan-yes-really-630451315ff2']\n",
            "     retrieved sources: ['https://clintonwhitehouse3.archives.gov/wh/new/pacific/tokyo.html']\n",
            "     result: hit\n",
            "\n",
            "[4] Query: When was OpenAI founded?\n",
            "     expected_source: en.wikipedia.org/wiki/OpenAI\n",
            "     urls fetched: ['https://en.wikipedia.org/wiki/OpenAI', 'https://www.lxahub.com/stories/the-history-of-openai', 'https://openai.com/our-structure/']\n",
            "     retrieved sources: ['https://www.lxahub.com/stories/the-history-of-openai']\n",
            "     result: hit\n",
            "\n",
            "[5] Query: What is LangChain used for?\n",
            "     expected_source: langchain.com\n",
            "     urls fetched: ['https://medium.com/around-the-prompt/what-is-langchain-and-why-should-i-care-as-a-developer-b2d952c42b28', 'https://aws.amazon.com/what-is/langchain/', 'https://python.langchain.com/docs/introduction/']\n",
            "     retrieved sources: ['https://aws.amazon.com/what-is/langchain/']\n",
            "     result: hit\n",
            "\n",
            "[6] Query: Which company created the Mistral-7B model?\n",
            "     expected_source: mistral.ai\n",
            "     urls fetched: ['https://en.wikipedia.org/wiki/Mistral_AI', 'https://mistral.ai/news/announcing-mistral-7b', 'https://medium.com/data-science/mistral-7b-explained-towards-more-efficient-language-models-7f9c6e6b7251']\n",
            "     retrieved sources: ['https://mistral.ai/news/announcing-mistral-7b']\n",
            "     result: hit\n",
            "\n",
            "[7] Query: What is FAISS used for?\n",
            "     expected_source: github.com/facebookresearch/faiss\n",
            "     urls fetched: ['https://ai.meta.com/tools/faiss/', 'https://www.pinecone.io/learn/series/faiss/faiss-tutorial/', 'https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/']\n",
            "     retrieved sources: ['https://ai.meta.com/tools/faiss/']\n",
            "     result: hit\n",
            "\n",
            "[8] Query: Who invented the World Wide Web?\n",
            "     expected_source: en.wikipedia.org/wiki/Tim_Berners-Lee\n",
            "     urls fetched: ['https://home.cern/science/computing/birth-web', 'https://en.wikipedia.org/wiki/Tim_Berners-Lee', 'https://home.cern/science/computing/birth-web/short-history-web']\n",
            "     retrieved sources: ['https://home.cern/science/computing/birth-web']\n",
            "     result: hit\n",
            "\n",
            "[9] Query: What does RAG stand for in AI?\n",
            "     expected_source: arxiv.org\n",
            "     urls fetched: ['https://aws.amazon.com/what-is/retrieval-augmented-generation/', 'https://cloud.google.com/use-cases/retrieval-augmented-generation', 'https://en.wikipedia.org/wiki/Retrieval-augmented_generation']\n",
            "     retrieved sources: ['https://aws.amazon.com/what-is/retrieval-augmented-generation/']\n",
            "     result: hit\n",
            "\n",
            "[10] Query: What is the default port for HTTP?\n",
            "     expected_source: developer.mozilla.org\n",
            "     urls fetched: ['https://superuser.com/questions/996828/why-was-port-80-chosen-as-the-default-http-port-and-443-as-the-default-https-por', 'https://www.cbtnuggets.com/blog/technology/networking/http-80-vs-https-443', 'https://docs.oracle.com/en/storage/tape-storage/sl4000/slklg/default-port-numbers.html']\n",
            "     retrieved sources: ['https://superuser.com/questions/996828/why-was-port-80-chosen-as-the-default-http-port-and-443-as-the-default-https-por']\n",
            "     result: hit\n",
            "\n",
            "[11] Query: What programming language is TensorFlow written in?\n",
            "     expected_source: tensorflow.org\n",
            "     urls fetched: ['https://stackoverflow.com/questions/35677724/tensorflow-why-was-python-the-chosen-language', 'https://en.wikipedia.org/wiki/TensorFlow', 'https://www.databricks.com/glossary/tensorflow-guide']\n",
            "     retrieved sources: ['https://www.databricks.com/glossary/tensorflow-guide']\n",
            "     result: hit\n",
            "\n",
            "[12] Query: What is the latest iPhone model?\n",
            "     expected_source: apple.com\n",
            "     urls fetched: ['https://www.apple.com/iphone/', 'https://www.apple.com/iphone-17-pro/', 'https://en.wikipedia.org/wiki/List_of_iPhone_models']\n",
            "     retrieved sources: ['https://www.apple.com/iphone/']\n",
            "     result: hit\n",
            "\n",
            "[13] Query: Which algorithm does Google use for search ranking?\n",
            "     expected_source: en.wikipedia.org/wiki/PageRank\n",
            "     urls fetched: ['https://en.wikipedia.org/wiki/PageRank', 'https://www.google.com/intl/en_us/search/howsearchworks/how-search-works/ranking-results', 'https://developers.google.com/search/docs/fundamentals/how-search-works']\n",
            "     retrieved sources: ['https://www.google.com/intl/en_us/search/howsearchworks/how-search-works/ranking-results', 'https://en.wikipedia.org/wiki/pagerank']\n",
            "     result: hit\n",
            "\n",
            "[14] Query: What is HuggingFace Transformers?\n",
            "     expected_source: huggingface.co/transformers\n",
            "     urls fetched: ['https://huggingface.co/docs/transformers/en/index', 'https://learn.microsoft.com/en-us/azure/databricks/machine-learning/train-model/huggingface/', 'https://github.com/huggingface/transformers']\n",
            "     retrieved sources: ['https://huggingface.co/docs/transformers/en/index', 'https://github.com/huggingface/transformers', 'https://learn.microsoft.com/en-us/azure/databricks/machine-learning/train-model/huggingface/']\n",
            "     result: miss\n",
            "\n",
            "[15] Query: When was ChatGPT released?\n",
            "     expected_source: en.wikipedia.org/wiki/ChatGPT\n",
            "     urls fetched: ['https://en.wikipedia.org/wiki/ChatGPT', 'https://www.reddit.com/r/cscareerquestions/comments/1j5xngz/its_almost_been_25_years_since_chatgpt_was_first/', 'https://www.forbes.com/sites/bernardmarr/2023/05/19/a-short-history-of-chatgpt-how-we-got-to-where-we-are-today/']\n",
            "     retrieved sources: ['https://www.forbes.com/sites/bernardmarr/2023/05/19/a-short-history-of-chatgpt-how-we-got-to-where-we-are-today/', 'https://en.wikipedia.org/wiki/chatgpt']\n",
            "     result: hit\n",
            "\n",
            "Retrieval accuracy @top-3: 93.33% (14/15)\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# Config variables (inline instead of config.py)\n",
        "# -----------------------------\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "You are a great researcher. With the information provided understand in deep and try to answer the question.\n",
        "If you cant answer the question based on the information either say you cant find an answer or unable to find an answer.\n",
        "So try to understand in depth about the context and answer only based on the information provided. Dont generate irrelevant answers.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Do provide only helpful answers\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "INPUT_VARIABLES = [\"context\", \"question\"]\n",
        "SEPARATORS = \"\\n\"\n",
        "CHUNK_SIZE = 3000\n",
        "CHUNK_OVERLAP = 500\n",
        "EMBEDDER = \"BAAI/bge-base-en-v1.5\"\n",
        "CHAIN_TYPE = \"stuff\"\n",
        "SEARCH_KWARGS = {'k': 3}\n",
        "\n",
        "# -----------------------------\n",
        "# Imports\n",
        "# -----------------------------\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "from typing import List\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.document_loaders import UnstructuredURLLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings  # updated\n",
        "\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "# -----------------------------\n",
        "# Researcher class (with evaluation)\n",
        "# -----------------------------\n",
        "class Researcher:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.serper_api_key = os.getenv(\"SERPER_API_KEY\")\n",
        "        self.groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "        self.prompt_template = PromptTemplate(\n",
        "            template=PROMPT_TEMPLATE,\n",
        "            input_variables=INPUT_VARIABLES\n",
        "        )\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            separators=SEPARATORS,\n",
        "            chunk_size=CHUNK_SIZE,\n",
        "            chunk_overlap=CHUNK_OVERLAP\n",
        "        )\n",
        "\n",
        "        # Use a current production Groq model\n",
        "        self.llm = ChatGroq(\n",
        "            temperature=0.5,\n",
        "            model_name=\"groq/compound\",\n",
        "            groq_api_key=self.groq_api_key\n",
        "        )\n",
        "\n",
        "        # CPU-friendly embeddings (safe for Colab free)\n",
        "        self.hfembeddings = HuggingFaceEmbeddings(\n",
        "            model_name=EMBEDDER,\n",
        "            model_kwargs={'device': 'cpu'}\n",
        "        )\n",
        "\n",
        "    def search_articles(self, query: str) -> dict:\n",
        "        url = \"https://google.serper.dev/search\"\n",
        "        data = json.dumps({\"q\": query})\n",
        "        headers = {\n",
        "            'X-API-KEY': self.serper_api_key,\n",
        "            'Content-Type': 'application/json'\n",
        "        }\n",
        "        response = requests.post(url, headers=headers, data=data)\n",
        "        response.raise_for_status()\n",
        "        return response.json()\n",
        "\n",
        "    def research_answerer(self):\n",
        "        research_qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=self.llm,\n",
        "            chain_type=CHAIN_TYPE,\n",
        "            retriever=self.db.as_retriever(search_kwargs=SEARCH_KWARGS),\n",
        "            return_source_documents=True,\n",
        "            verbose=True,\n",
        "            chain_type_kwargs={\"prompt\": self.prompt_template}\n",
        "        )\n",
        "        return research_qa_chain\n",
        "\n",
        "    def get_urls(self, articles: dict) -> List[str]:\n",
        "        urls = []\n",
        "        if not articles:\n",
        "            return urls\n",
        "        try:\n",
        "            if \"answerBox\" in articles and \"link\" in articles[\"answerBox\"]:\n",
        "                urls.append(articles[\"answerBox\"][\"link\"])\n",
        "        except Exception:\n",
        "            pass\n",
        "        for i in range(0, min(3, len(articles.get(\"organic\", [])))):\n",
        "            urls.append(articles[\"organic\"][i][\"link\"])\n",
        "        return urls\n",
        "\n",
        "    def get_content_from_urls(self, urls: List[str]):\n",
        "        # load pages one by one to be robust and skip failures\n",
        "        documents = []\n",
        "        for url in urls:\n",
        "            try:\n",
        "                loader = UnstructuredURLLoader(urls=[url])\n",
        "                docs = loader.load()\n",
        "                # attach url to metadata if loader didn't\n",
        "                for d in docs:\n",
        "                    if not d.metadata.get(\"source\"):\n",
        "                        d.metadata[\"source\"] = url\n",
        "                documents.extend(docs)\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping URL {url}: {e}\")\n",
        "        return documents\n",
        "\n",
        "    def research_given_query(self, research_objective: str, research_content):\n",
        "        docs = self.text_splitter.split_documents(research_content)\n",
        "        self.db = FAISS.from_documents(documents=docs, embedding=self.hfembeddings)\n",
        "        bot = self.research_answerer()\n",
        "        # use invoke if your LangChain version expects it\n",
        "        try:\n",
        "            research_out = bot.invoke({\"query\": research_objective})\n",
        "        except Exception:\n",
        "            research_out = bot({\"query\": research_objective})\n",
        "        return research_out[\"result\"]\n",
        "\n",
        "    def research(self, query: str):\n",
        "        search_articles = self.search_articles(query)\n",
        "        urls = self.get_urls(search_articles)\n",
        "        research_content = self.get_content_from_urls(urls)\n",
        "        answer = self.research_given_query(query, research_content)\n",
        "        return answer\n",
        "\n",
        "    # -----------------------------\n",
        "    # New: evaluate retrieval accuracy\n",
        "    # -----------------------------\n",
        "    def evaluate_retriever(self, dataset: List[dict], top_k: int = 3):\n",
        "        \"\"\"\n",
        "        dataset: list of {\"query\":..., \"expected_source\":..., \"expected_answer\":...}\n",
        "        This runs the same search -> scrape -> embed -> faiss -> retrieve pipeline and\n",
        "        checks whether expected_source or expected_answer snippet appears in top-k docs.\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        correct = 0\n",
        "        total = len(dataset)\n",
        "\n",
        "        for i, sample in enumerate(dataset, start=1):\n",
        "            q = sample[\"query\"]\n",
        "            expected_source = sample.get(\"expected_source\", \"\").lower()\n",
        "            expected_answer = sample.get(\"expected_answer\", \"\").lower()\n",
        "\n",
        "            # Run pipeline: search -> get urls -> load content -> build FAISS\n",
        "            try:\n",
        "                search_res = self.search_articles(q)\n",
        "            except Exception as e:\n",
        "                print(f\"[{i}] Search failed for query: {q} -> {e}\")\n",
        "                results.append((q, False, [], \"search_failed\"))\n",
        "                continue\n",
        "\n",
        "            urls = self.get_urls(search_res)\n",
        "            docs = self.get_content_from_urls(urls)\n",
        "            if not docs:\n",
        "                print(f\"[{i}] No documents loaded for query: {q} (urls: {urls})\")\n",
        "                results.append((q, False, urls, \"no_docs\"))\n",
        "                continue\n",
        "\n",
        "            # build faiss for these docs\n",
        "            db = FAISS.from_documents(documents=docs, embedding=self.hfembeddings)\n",
        "            retriever = db.as_retriever(search_kwargs={\"k\": top_k})\n",
        "\n",
        "            # retrieve\n",
        "            try:\n",
        "                retrieved = retriever.get_relevant_documents(q)\n",
        "            except Exception as e:\n",
        "                print(f\"[{i}] Retrieval failed for query: {q} -> {e}\")\n",
        "                results.append((q, False, [], \"retrieval_failed\"))\n",
        "                continue\n",
        "\n",
        "            # collect retrieved sources/text\n",
        "            retrieved_sources = []\n",
        "            retrieved_texts = []\n",
        "            found = False\n",
        "            for d in retrieved:\n",
        "                src = \"\"\n",
        "                # try common metadata fields\n",
        "                for key in (\"source\", \"url\", \"source_url\"):\n",
        "                    if key in d.metadata and d.metadata[key]:\n",
        "                        src = str(d.metadata[key]).lower()\n",
        "                        break\n",
        "                retrieved_sources.append(src)\n",
        "                content = (d.page_content or \"\").lower()\n",
        "                retrieved_texts.append(content)\n",
        "\n",
        "                # check for expected_source substring\n",
        "                if expected_source and expected_source in src:\n",
        "                    found = True\n",
        "                    break\n",
        "                # fallback: check expected_answer snippet in content\n",
        "                if expected_answer and expected_answer in content:\n",
        "                    found = True\n",
        "                    break\n",
        "\n",
        "            if found:\n",
        "                correct += 1\n",
        "                status = \"hit\"\n",
        "            else:\n",
        "                status = \"miss\"\n",
        "\n",
        "            results.append((q, found, urls, retrieved_sources, status))\n",
        "\n",
        "            # print per-query summary\n",
        "            print(f\"[{i}] Query: {q}\")\n",
        "            print(f\"     expected_source: {sample.get('expected_source')}\")\n",
        "            print(f\"     urls fetched: {urls}\")\n",
        "            print(f\"     retrieved sources: {retrieved_sources}\")\n",
        "            print(f\"     result: {status}\\n\")\n",
        "\n",
        "        accuracy = correct / total * 100 if total else 0.0\n",
        "        print(f\"Retrieval accuracy @top-{top_k}: {accuracy:.2f}% ({correct}/{total})\")\n",
        "        return accuracy, results\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Test dataset (15 queries)\n",
        "# -----------------------------\n",
        "TEST_DATA = [\n",
        "    {\"query\": \"What is the latest version of Python?\", \"expected_answer\": \"python 3.13\", \"expected_source\": \"python.org\"},\n",
        "    {\"query\": \"Who is the CEO of NVIDIA?\", \"expected_answer\": \"jensen huang\", \"expected_source\": \"nvidia.com\"},\n",
        "    {\"query\": \"What is the capital of Japan?\", \"expected_answer\": \"tokyo\", \"expected_source\": \"en.wikipedia.org/wiki/Tokyo\"},\n",
        "    {\"query\": \"When was OpenAI founded?\", \"expected_answer\": \"december 2015\", \"expected_source\": \"en.wikipedia.org/wiki/OpenAI\"},\n",
        "    {\"query\": \"What is LangChain used for?\", \"expected_answer\": \"framework for building applications\", \"expected_source\": \"langchain.com\"},\n",
        "    {\"query\": \"Which company created the Mistral-7B model?\", \"expected_answer\": \"mistral ai\", \"expected_source\": \"mistral.ai\"},\n",
        "    {\"query\": \"What is FAISS used for?\", \"expected_answer\": \"similarity search\", \"expected_source\": \"github.com/facebookresearch/faiss\"},\n",
        "    {\"query\": \"Who invented the World Wide Web?\", \"expected_answer\": \"tim berners-lee\", \"expected_source\": \"en.wikipedia.org/wiki/Tim_Berners-Lee\"},\n",
        "    {\"query\": \"What does RAG stand for in AI?\", \"expected_answer\": \"retrieval-augmented generation\", \"expected_source\": \"arxiv.org\"},\n",
        "    {\"query\": \"What is the default port for HTTP?\", \"expected_answer\": \"port 80\", \"expected_source\": \"developer.mozilla.org\"},\n",
        "    {\"query\": \"What programming language is TensorFlow written in?\", \"expected_answer\": \"c++\", \"expected_source\": \"tensorflow.org\"},\n",
        "    {\"query\": \"What is the latest iPhone model?\", \"expected_answer\": \"iphone 16\", \"expected_source\": \"apple.com\"},\n",
        "    {\"query\": \"Which algorithm does Google use for search ranking?\", \"expected_answer\": \"pagerank\", \"expected_source\": \"en.wikipedia.org/wiki/PageRank\"},\n",
        "    {\"query\": \"What is HuggingFace Transformers?\", \"expected_answer\": \"library for state-of-the-art natural language processing models\", \"expected_source\": \"huggingface.co/transformers\"},\n",
        "    {\"query\": \"When was ChatGPT released?\", \"expected_answer\": \"november 2022\", \"expected_source\": \"en.wikipedia.org/wiki/ChatGPT\"}\n",
        "]\n",
        "\n",
        "# -----------------------------\n",
        "# Run evaluation if executed directly\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Provide your API keys here (or ensure .env is present)\n",
        "    os.environ[\"SERPER_API_KEY\"] = os.getenv(\"SERPER_API_KEY\", os.environ.get(\"SERPER_API_KEY\", \"\"))\n",
        "    os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", os.environ.get(\"GROQ_API_KEY\", \"\"))\n",
        "\n",
        "    researcher = Researcher()\n",
        "\n",
        "    # Run the retriever-only evaluation (does NOT invoke the LLM for scoring)\n",
        "    acc, details = researcher.evaluate_retriever(TEST_DATA, top_k=3)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
